{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23300,"status":"ok","timestamp":1714893388628,"user":{"displayName":"Vinitra","userId":"07269198206276965478"},"user_tz":240},"id":"tz4GjH2tQCVy","outputId":"af9cc2f9-9a37-409c-aeb4-fba08fee1c1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#mount drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":960,"status":"ok","timestamp":1714893412102,"user":{"displayName":"Vinitra","userId":"07269198206276965478"},"user_tz":240},"id":"IJYf-GAtQzLp","outputId":"ff43b892-5d8b-43a3-d544-156e3ad8144c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/UMass-Courses/Semester-2/CS670/UMass-CS670-Project\n","common\t\t\t  experiments\t\t   last_run_info.json\t  source-data\n","config.yaml\t\t  get_dataset.sh\t   models\t\t  texture_params.txt\n","datautils\t\t  github-repo-clone.ipynb  preprocess_dataset.py\n","demo\t\t\t  Index.ipynb\t\t   README.md\n","excluded-train-files.txt  index.py\t\t   SINet.ipynb\n"]}],"source":["# move into project directory\n","repo_name = \"UMass-CS670-Project\"\n","%cd /content/drive/MyDrive/UMass-Courses/Semester-2/CS670/$repo_name\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1713148079902,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"},"user_tz":240},"id":"rnSym0W0Rmz-","outputId":"69dfbd62-7d55-4ac5-d047-667bfaca25b0"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n!pip3 install torch torchvision torchaudio\\n!pip install matplotlib numpy pandas pyyaml opencv-python\\n'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# set up environment\n","# comment out if not required\n","'''\n","!pip3 install torch torchvision torchaudio\n","!pip install matplotlib numpy pandas pyyaml opencv-python\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvSx-iP1P0Rx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714790015886,"user_tz":240,"elapsed":365382,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}},"outputId":"e074ae2f-018e-42f0-f0bb-45194b215e50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["# this cell is for downloading data.\n","# as of yet data is not hosted and is available in the private data folder\n","# uncomment the lines below  to download the data\n","\n","import os\n","\n","!pip install gdown\n","\n","data_dir=\"source-data\"\n","\n","if not(os.path.exists(\"source-data\")):\n","    #!gdown https://drive.google.com/uc?id=18oBjWeuw5qAq4HG_jZdjUHas4APy-KJE&export=download\n","    #echo 'Downloaded data! Unzipping to data folder'\n","    !unzip -qq -d . ./COD10K-v3.zip\n","    os.rename(\"./COD10K-v3.zip\", \"./source-data/COD10K-v3.zip\")\n","else:\n","    print(\"\\nThe data directory exists!\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"BboU3GhFP0R1","executionInfo":{"status":"ok","timestamp":1714894170670,"user_tz":240,"elapsed":13026,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}}},"outputs":[],"source":["# setup some imports\n","from datautils.datareader import read_data\n","from datautils.dataset import COD10KDataset\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","import torch\n","import argparse\n","from tqdm import tqdm\n","from torchvision.io import read_image\n","from torchvision.utils import save_image\n","from torchvision.transforms.functional import to_pil_image\n","import json\n","\n","from experiments.style_transfer import style_transfer\n","from common.visualizer import layer_visualizer\n","from common.utils import init_config"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"q7unXlk9haYV","executionInfo":{"status":"ok","timestamp":1714894170671,"user_tz":240,"elapsed":4,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}}},"outputs":[],"source":["seed = 123\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","source":["config_params = init_config()\n","print(config_params)"],"metadata":{"id":"oPg2Qi7OUBPz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714894170832,"user_tz":240,"elapsed":164,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}},"outputId":"863bd3e7-f233-442e-b490-0c585ef77434"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'data_dir': '/content/drive/MyDrive/UMass-Courses/Semester-2/CS670/UMass-CS670-Project/source-data', 'device': 'cpu', 'root_dir': '/content/drive/MyDrive/UMass-Courses/Semester-2/CS670/UMass-CS670-Project', 'use_gpu': False}\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwGytYYtenxa"},"outputs":[],"source":["def run_style_transfer_pipeline(args, texture_name, style_weight, last_batch_run = -1):\n","    pos_data_paths = read_data('Train')\n","\n","    dataset = COD10KDataset(pos_data_paths)\n","\n","    dataloader = DataLoader(dataset, batch_size = args.batch_size, shuffle = False)\n","\n","    for i_batch, batch in enumerate(dataloader):\n","        if i_batch > last_batch_run:\n","            print(f\"Processing batch {i_batch}, image: {batch['img_name']} of dimensions: {batch['img'].shape}\")\n","            style_img = read_image(f'./source-data/Textures/{texture_name}.jpg')\n","            new_img = style_transfer(\n","                batch['img'],\n","                style_img,\n","                [0, 2, 5, 14, 23],\n","                21,\n","                1e-4,\n","                [style_weight]*5,\n","                1e-5,\n","                args)\n","            img_name = batch['img_name'][0]\n","            img_name = img_name.replace(\".jpg\", \"\")\n","            img_name = f\"./source-data/Train/Styled-Image/{img_name}-Texture-{texture_name}.jpg\"\n","            save_image(new_img, img_name)\n","            last_run = {\n","                \"last_batch\": i_batch\n","            }\n","            print(f\"completing transfer of img {i_batch} with texture {texture_name}\")\n","            with open(\"./last_run_info.json\", \"w\") as fp:\n","                json.dump(last_run, fp)\n","\n","    print(f'Finished modifying train dataset images for {texture_name}')\n","    last_run = {\n","        \"last_batch\": -1\n","    }\n","    with open(\"./last_run_info.json\", \"w\") as fp:\n","        json.dump(last_run, fp)\n","    return -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoM2h30rU6FQ"},"outputs":[],"source":["\n","param_dict = {\n","    'moss': {\n","        'epochs': 100,\n","        'style_weights': 0.5\n","    },\n","    'snow': {\n","        'epochs': 200,\n","        'style_weights': 1.5\n","    },\n","    'wet-sand': {\n","        'epochs': 200,\n","        'style_weights': 3.5\n","    },\n","    'sand-with-moss': {\n","        'epochs': 100,\n","        'style_weights': 2.5\n","    },\n","    'rain': {\n","        'epochs': 100,\n","        'style_weights': 4.5\n","    },\n","    'wood': {\n","        'epochs': 100,\n","        'style_weights': 0.5\n","    },\n","    'grass': {\n","        'epochs': 100,\n","        'style_weights': 0.1\n","    },\n","    'foliage-texture': {\n","        'epochs': 100,\n","        'style_weights': 0.5\n","    },\n","    'blue-coral': {\n","        'epochs': 100,\n","        'style_weights': 1.1\n","    }\n","}\n","\n","with open(\"./last_run_info.json\") as fp:\n","    last_run = json.load(fp)[\"last_batch\"]\n","\n","selected_textures = ['moss', 'wet-sand', 'sand-with-moss', 'rain', 'foliage-texture']\n","\n","for texture in selected_textures:\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--batch_size', type = int, default = 1)\n","    parser.add_argument('--model_name', type=str, default='vgg')\n","    parser.add_argument('--lr', type=float, default=0.05)\n","    parser.add_argument('--max_iter', type=int, default=param_dict[texture]['epochs'])\n","    args = parser.parse_args(args=[])\n","\n","    #style_img = read_image(f'./data/Textures/{texture}.jpg')\n","    #print(img.size())\n","    #layer_visualizer(img, args)\n","\n","    run_style_transfer_pipeline(args, texture, param_dict[texture]['style_weights'], last_run)"]},{"cell_type":"code","source":["from datautils.datareader import read_data\n","from torch.utils.data import DataLoader\n","from datautils.dataset import COD10KDataset\n","from models.custom_models import get_model\n","from torchvision.transforms import transforms\n","import torch\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torch.utils.data import Subset\n","from random import shuffle\n","from tqdm import tqdm\n","import os\n","\n","from common.utils import get_config, save_experiment_output, save_model_helpers, load_model, get_modelinfo\n","\n","def image_collate(batch):\n","    batchlist = list(map(list, zip(*batch)))\n","    return batchlist\n","\n","def __convert_to_grascale(img):\n","    imin, imax = img.min(), img.max()\n","    x = (img - imin) / (imax - imin)\n","    return x\n","\n","def __prepare_data(type = 'Train'):\n","    data_paths = read_data('Train', True)\n","    dataset = COD10KDataset(data_paths, 'binary_classification')\n","    smlen = int(0.1 * len(dataset))\n","    smftr_dataset = Subset(dataset, list(range(smlen)))\n","    return dataset, smftr_dataset\n","\n","def __get_experiment_chkpt(model, optimizer):\n","\n","    cfg = get_config()\n","    root_dir = cfg[\"root_dir\"]\n","\n","    mpath = os.path.join(root_dir, \"models/checkpoints/curr_model.pt\")\n","    opath = os.path.join(root_dir, \"models/checkpoints/curr_model_optimizer.pt\")\n","    if os.path.exists(mpath):\n","        print(\"Loading saved model\")\n","        saved_model = load_model(mpath)\n","        saved_optim = load_model(opath)\n","        #model_dict = saved_model[\"model_state\"]\n","        model.load_state_dict(saved_model)\n","        optimizer.load_state_dict(saved_optim)\n","        model_info = get_modelinfo(True)\n","        return model, optimizer, model_info\n","    else:\n","        return model, optimizer, None\n","\n","def __run_train_loop(args, model, optimizer, model_info, train_dataloader, val_dataloader):\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize((args.resize_size, args.resize_size)),\n","        transforms.Lambda(__convert_to_grascale),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    loss_fn = torch.nn.BCELoss()\n","    last_epoch = -1\n","    train_losses = []\n","    val_losses = []\n","    val_accs = []\n","    if model_info != None:\n","        last_epoch = model_info['last_epoch']\n","        train_losses, val_losses, val_accs = model_info['trlosshistory'], model_info['vallosshistory'], model_info['valacchistory']\n","        epoch_arr = list(range(last_epoch + 1, args.max_iter))\n","    else:\n","        epoch_arr = list(range(args.max_iter))\n","\n","    for epoch_i in epoch_arr:\n","        avg_loss = 0\n","        print(f'Running epoch {epoch_i}')\n","        for i_batch, batch in enumerate(tqdm(train_dataloader, desc = '\\tRunning through training set', position = 0, leave = True)):\n","            #print(batch[1])\n","            lbls = torch.tensor(batch[1])\n","            #inds = torch.tensor(list(range(len(batch[1]))))\n","            #print(lbls.size(), lbls == 0)\n","            target = torch.zeros((len(batch[1]), 2))\n","            target[lbls == 1, 1] = 1.0\n","            target[lbls == 0, 0] = 0.0\n","            #print(target.size())\n","            #print(target)\n","            optimizer.zero_grad()\n","            #print(len(batch), type(batch[0]), len(batch[0]))\n","            img_batch = list(map(transform, batch[0]))\n","            img_batch = torch.stack(img_batch, 0)\n","            #print(img_batch.size())\n","            #tr_batch = transform(batch['img'])\n","            op = F.softmax(model(img_batch), 1)\n","            loss = loss_fn(op, target)\n","            loss.backward()\n","            avg_loss += loss.item()\n","            optimizer.step()\n","            #print(op.size())\n","            #break\n","\n","        train_losses.append(avg_loss / (i_batch + 1))\n","\n","        with torch.no_grad():\n","            val_acc = 0.0\n","            val_loss = 0.0\n","            for i_batch, batch in enumerate(tqdm(val_dataloader, desc = '\\tRunning through training set', position = 0, leave = True)):\n","                lbls = torch.tensor(batch[1])\n","                target = torch.zeros((len(batch[1]), 2))\n","                target[lbls == 1, 1] = 1.0\n","                target[lbls == 0, 0] = 0.0\n","\n","                img_batch = list(map(transform, batch[0]))\n","                img_batch = torch.stack(img_batch, 0)\n","                #print(img_batch.size())\n","                #tr_batch = transform(batch['img'])\n","                op = model(img_batch)\n","                loss = loss_fn(F.softmax(op, 1), target)\n","                lbls = torch.argmax(op, 1)\n","                val_acc += torch.eq(lbls, torch.tensor(batch[1])).sum()\n","                val_loss += loss.item()\n","            val_acc /= (i_batch + 1)\n","            val_loss /= (i_batch + 1)\n","            val_losses.append(val_loss)\n","            val_accs.append(val_acc.item())\n","        #print(train_losses, val_losses, val_accs)\n","        chkpt_info = {\n","            'trlosshistory': train_losses,\n","            'vallosshistory': val_losses,\n","            'valacchistory': val_accs,\n","            'last_epoch': epoch_i\n","        }\n","        save_experiment_output(model, chkpt_info)\n","        save_model_helpers(optimizer.state_dict())\n","    save_experiment_output(model, chkpt_info, False)\n","    save_model_helpers(optimizer.state_dict(), False)\n","    plt.plot(list(range(args.max_iter)), train_losses, color = 'red')\n","    plt.plot(list(range(args.max_iter)), val_losses, color = 'blue')\n","    plt.legend(['Training loss', 'Validation loss'])\n","    plt.title('Loss - Epoch plot')\n","    plt.show()\n","    plt.clf()\n","    plt.plot(list(range(args.max_iter)), val_accs)\n","    plt.title('Validation accuracy plot')\n","    plt.show()\n","\n","\n","def binary_classification(args):\n","    full_dataset, smftr_dataset = __prepare_data()\n","    smlen = len(smftr_dataset)\n","    idxs = list(range(smlen))\n","    shuffle(idxs)\n","    vlen = int(0.2 * smlen)\n","    val_idxs = idxs[:vlen]\n","    tr_idxs = idxs[vlen:]\n","    train_dataset = Subset(smftr_dataset, tr_idxs)\n","    val_dataset = Subset(smftr_dataset, val_idxs)\n","    #train_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = image_collate)\n","    train_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = image_collate)\n","    val_dataloader = DataLoader(val_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = image_collate)\n","    model = get_model(2, args.model_name)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr, betas = (0.99, 0.999))\n","\n","    model, optimizer, model_info = __get_experiment_chkpt(model, optimizer)\n","\n","    __run_train_loop(args, model, optimizer, model_info, train_dataloader, val_dataloader)\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--batch_size', type = int, default = 16)\n","parser.add_argument('--model_name', type=str, default='vit')\n","parser.add_argument('--lr', type=float, default=0.01)\n","parser.add_argument('--max_iter', type=int, default=5)\n","parser.add_argument('--resize_size', type=int, default=224)\n","args = parser.parse_args(args=[])\n","\n","binary_classification(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXi84P_vPS9c","outputId":"50f2ca6c-a91a-4bc3-bf5d-cdad4ab7810e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running epoch 0\n"]},{"output_type":"stream","name":"stderr","text":["\tRunning through training set: 100%|██████████| 30/30 [10:37<00:00, 21.25s/it]\n","\tRunning through training set: 100%|██████████| 8/8 [00:52<00:00,  6.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Running epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["\tRunning through training set:  47%|████▋     | 14/30 [07:20<12:45, 47.87s/it]"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}