{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22455,"status":"ok","timestamp":1714828415879,"user":{"displayName":"Vinitra","userId":"07269198206276965478"},"user_tz":240},"id":"tz4GjH2tQCVy","outputId":"c94caebd-db20-4434-d09a-f3f552b13eed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#mount drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":752,"status":"ok","timestamp":1714828417078,"user":{"displayName":"Vinitra","userId":"07269198206276965478"},"user_tz":240},"id":"IJYf-GAtQzLp","outputId":"b03137de-bb4a-4918-d6ee-f96841c92d35"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/UMass-Courses/Semester-2/CS670/UMass-CS670-Project\n","common\t\t\t  experiments\t\t   index.py\t\t  README.md\n","datautils\t\t  get_dataset.sh\t   last_run_info.json\t  SINet.ipynb\n","demo\t\t\t  github-repo-clone.ipynb  models\t\t  source-data\n","excluded-train-files.txt  Index.ipynb\t\t   preprocess_dataset.py  texture_params.txt\n"]}],"source":["# move into project directory\n","repo_name = \"UMass-CS670-Project\"\n","%cd /content/drive/MyDrive/UMass-Courses/Semester-2/CS670/$repo_name\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1713148079902,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"},"user_tz":240},"id":"rnSym0W0Rmz-","outputId":"69dfbd62-7d55-4ac5-d047-667bfaca25b0"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n!pip3 install torch torchvision torchaudio\\n!pip install matplotlib numpy pandas pyyaml opencv-python\\n'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# set up environment\n","# comment out if not required\n","'''\n","!pip3 install torch torchvision torchaudio\n","!pip install matplotlib numpy pandas pyyaml opencv-python\n","'''"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VvSx-iP1P0Rx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714790015886,"user_tz":240,"elapsed":365382,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}},"outputId":"e074ae2f-018e-42f0-f0bb-45194b215e50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["# this cell is for downloading data.\n","# as of yet data is not hosted and is available in the private data folder\n","# uncomment the lines below  to download the data\n","\n","import os\n","\n","!pip install gdown\n","\n","data_dir=\"source-data\"\n","\n","if not(os.path.exists(\"source-data\")):\n","    #!gdown https://drive.google.com/uc?id=18oBjWeuw5qAq4HG_jZdjUHas4APy-KJE&export=download\n","    #echo 'Downloaded data! Unzipping to data folder'\n","    !unzip -qq -d . ./COD10K-v3.zip\n","    os.rename(\"./COD10K-v3.zip\", \"./source-data/COD10K-v3.zip\")\n","else:\n","    print(\"\\nThe data directory exists!\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"BboU3GhFP0R1","executionInfo":{"status":"ok","timestamp":1714828425999,"user_tz":240,"elapsed":8924,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}}},"outputs":[],"source":["# setup some imports\n","from datautils.datareader import read_data\n","from datautils.dataset import COD10KDataset\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","import torch\n","import argparse\n","from tqdm import tqdm\n","from torchvision.io import read_image\n","from torchvision.utils import save_image\n","from torchvision.transforms.functional import to_pil_image\n","import json\n","\n","from experiments.style_transfer import style_transfer\n","from common.visualizer import layer_visualizer"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"q7unXlk9haYV","executionInfo":{"status":"ok","timestamp":1714828425999,"user_tz":240,"elapsed":21,"user":{"displayName":"Vinitra","userId":"07269198206276965478"}}},"outputs":[],"source":["seed = 123\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwGytYYtenxa"},"outputs":[],"source":["def run_style_transfer_pipeline(args, texture_name, style_weight, last_batch_run = -1):\n","    pos_data_paths = read_data('Train')\n","\n","    dataset = COD10KDataset(pos_data_paths)\n","\n","    dataloader = DataLoader(dataset, batch_size = args.batch_size, shuffle = False)\n","\n","    for i_batch, batch in enumerate(dataloader):\n","        if i_batch > last_batch_run:\n","            print(f\"Processing batch {i_batch}, image: {batch['img_name']} of dimensions: {batch['img'].shape}\")\n","            style_img = read_image(f'./source-data/Textures/{texture_name}.jpg')\n","            new_img = style_transfer(\n","                batch['img'],\n","                style_img,\n","                [0, 2, 5, 14, 23],\n","                21,\n","                1e-4,\n","                [style_weight]*5,\n","                1e-5,\n","                args)\n","            img_name = batch['img_name'][0]\n","            img_name = img_name.replace(\".jpg\", \"\")\n","            img_name = f\"./source-data/Train/Styled-Image/{img_name}-Texture-{texture_name}.jpg\"\n","            save_image(new_img, img_name)\n","            last_run = {\n","                \"last_batch\": i_batch\n","            }\n","            print(f\"completing transfer of img {i_batch} with texture {texture_name}\")\n","            with open(\"./last_run_info.json\", \"w\") as fp:\n","                json.dump(last_run, fp)\n","\n","    print(f'Finished modifying train dataset images for {texture_name}')\n","    last_run = {\n","        \"last_batch\": -1\n","    }\n","    with open(\"./last_run_info.json\", \"w\") as fp:\n","        json.dump(last_run, fp)\n","    '''\n","    style_img = read_image(f'./data/Textures/{texture_name}.jpg')\n","    content_img = read_image('./data/Mini-Set/butterfly-image.jpg')\n","    #content_img = batch['img']\n","    print('b4 trans', content_img.shape)\n","    new_img = style_transfer(content_img,\n","            style_img,\n","            [0, 2, 5, 14, 23],\n","            21,\n","            1e-4,\n","            [style_weight]*5,\n","            1e-5,\n","            args)\n","    #pi = to_pil_image(pi)\n","    #plt.axis(\"off\")\n","    #plt.imshow(pi)\n","    #plt.show()\n","    save_image(new_img, f'./data/COD10K-v3/Train/Styled-Image/butterfly-image-Texture-{texture_name}.jpg')\n","    #print('after trans', new_img.size())\n","    '''\n","    return -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoM2h30rU6FQ"},"outputs":[],"source":["\n","param_dict = {\n","    'moss': {\n","        'epochs': 100,\n","        'style_weights': 0.5\n","    },\n","    'snow': {\n","        'epochs': 200,\n","        'style_weights': 1.5\n","    },\n","    'wet-sand': {\n","        'epochs': 200,\n","        'style_weights': 3.5\n","    },\n","    'sand-with-moss': {\n","        'epochs': 100,\n","        'style_weights': 2.5\n","    },\n","    'rain': {\n","        'epochs': 100,\n","        'style_weights': 4.5\n","    },\n","    'wood': {\n","        'epochs': 100,\n","        'style_weights': 0.5\n","    },\n","    'grass': {\n","        'epochs': 100,\n","        'style_weights': 0.1\n","    },\n","    'foliage-texture': {\n","        'epochs': 100,\n","        'style_weights': 0.5\n","    },\n","    'blue-coral': {\n","        'epochs': 100,\n","        'style_weights': 1.1\n","    }\n","}\n","\n","with open(\"./last_run_info.json\") as fp:\n","    last_run = json.load(fp)[\"last_batch\"]\n","\n","selected_textures = ['moss', 'wet-sand', 'sand-with-moss', 'rain', 'foliage-texture']\n","\n","for texture in selected_textures:\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--batch_size', type = int, default = 1)\n","    parser.add_argument('--model_name', type=str, default='vgg')\n","    parser.add_argument('--lr', type=float, default=0.05)\n","    parser.add_argument('--max_iter', type=int, default=param_dict[texture]['epochs'])\n","    args = parser.parse_args(args=[])\n","\n","    #style_img = read_image(f'./data/Textures/{texture}.jpg')\n","    #print(img.size())\n","    #layer_visualizer(img, args)\n","\n","    run_style_transfer_pipeline(args, texture, param_dict[texture]['style_weights'], last_run)"]},{"cell_type":"code","source":["from datautils.datareader import read_data\n","from torch.utils.data import DataLoader\n","from datautils.dataset import COD10KDataset\n","from models.custom_models import get_model\n","from torchvision.transforms import transforms\n","import torch\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torch.utils.data import Subset\n","from random import shuffle\n","\n","def image_collate(batch):\n","    batchlist = list(map(list, zip(*batch)))\n","    return batchlist\n","\n","def __convert_to_grascale(img):\n","    imin, imax = img.min(), img.max()\n","    x = (img - imin) / (imax - imin)\n","    return x\n","\n","def __prepare_data(type = 'Train'):\n","    data_paths = read_data('Train', True)\n","    dataset = COD10KDataset(data_paths, 'binary_classification')\n","    smlen = int(0.1 * len(dataset))\n","    smftr_dataset = Subset(dataset, list(range(smlen)))\n","    return dataset, smftr_dataset\n","\n","def binary_classification(args):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize((args.resize_size, args.resize_size)),\n","        transforms.Lambda(__convert_to_grascale),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225])\n","    ])\n","    full_dataset, smftr_dataset = __prepare_data()\n","    smlen = len(smftr_dataset)\n","    idxs = list(range(smlen))\n","    shuffle(idxs)\n","    vlen = int(0.2 * smlen)\n","    val_idxs = idxs[:vlen]\n","    tr_idxs = idxs[vlen:]\n","    train_dataset = Subset(smftr_dataset, tr_idxs)\n","    val_dataset = Subset(smftr_dataset, val_idxs)\n","    #train_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = image_collate)\n","    train_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = image_collate)\n","    val_dataloader = DataLoader(val_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = image_collate)\n","    model = get_model(2, \"vit\")\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr, betas = (0.99, 0.999))\n","    loss_fn = torch.nn.BCELoss()\n","    train_losses = []\n","\n","    for _ in range(args.max_iter):\n","        avg_loss = 0\n","        for i_batch, batch in enumerate(train_dataloader):\n","            #print(batch[1])\n","            lbls = torch.tensor(batch[1])\n","            #inds = torch.tensor(list(range(len(batch[1]))))\n","            #print(lbls.size(), lbls == 0)\n","            target = torch.zeros((len(batch[1]), 2))\n","            target[lbls == 1, 1] = 1.0\n","            target[lbls == 0, 0] = 0.0\n","            #print(target.size())\n","            #print(target)\n","            optimizer.zero_grad()\n","            #print(len(batch), type(batch[0]), len(batch[0]))\n","            img_batch = list(map(transform, batch[0]))\n","            img_batch = torch.stack(img_batch, 0)\n","            #print(img_batch.size())\n","            #tr_batch = transform(batch['img'])\n","            op = F.softmax(model(img_batch))\n","            loss = loss_fn(op, target)\n","            loss.backward()\n","            avg_loss += loss.item()\n","            #print(op.size())\n","            #break\n","        train_losses.append(avg_loss / (i_batch + 1))\n","    val_acc = 0.0\n","    for i_batch, batch in enumerate(val_dataloader):\n","        img_batch = list(map(transform, batch[0]))\n","        img_batch = torch.stack(img_batch, 0)\n","        #print(img_batch.size())\n","        #tr_batch = transform(batch['img'])\n","        op = model(img_batch)\n","        lbls = torch.argmax(op, 1)\n","        val_acc += torch.eq(lbls, torch.tensor(batch[1])).sum()\n","    val_acc /= vlen\n","\n","    plt.plot(list(range(args.max_iter)), train_losses)\n","    plt.title('Loss - Epoch plot')\n","    plt.show()\n","    print('Accuracy on validation set', val_acc)\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--batch_size', type = int, default = 16)\n","parser.add_argument('--model_name', type=str, default='vit')\n","parser.add_argument('--lr', type=float, default=0.01)\n","parser.add_argument('--max_iter', type=int, default=10)\n","parser.add_argument('--resize_size', type=int, default=224)\n","args = parser.parse_args(args=[])\n","\n","binary_classification(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXi84P_vPS9c","outputId":"4bf3e301-dd23-4c3f-b725-1f8ce3ed5961"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-32be52549f78>:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  op = F.softmax(model(img_batch))\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}